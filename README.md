# åŸºäºSIFTå®ç°å›¾åƒæ‹¼æ¥ï¼ˆA look into the pastï¼‰
æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹ï¼š
1. å…³é”®ç‚¹æ£€æµ‹ï¼ˆKeypoints Detectionï¼‰
2. åŒ¹é…ï¼ˆMatchï¼‰
3. æ‹¼æ¥ï¼ˆImage Stitichï¼‰

æˆå‘˜åŠåˆ†å·¥ï¼š
1. ç‹å¿—å¼º PB18051049
    - ç®—æ³•è®¾è®¡
    - ä»£ç å®ç°
    - æ–‡æ¡£æ’°å†™
2. è’²æ˜æ˜± PB18111733
    - æ–¹æ¡ˆè°ƒç ”
    - ç®—æ³•è®¾è®¡
    - æ–‡æ¡£æ’°å†™

## é—®é¢˜æè¿°
1. åˆè¡·å’ŒåŠ¨æœºï¼š[A look into the past](https://blog.flickr.net/en/2010/01/27/a-look-into-the-past)æ˜¯ä¸€ç§å›¾ç‰‡è‰ºæœ¯ï¼Œè®©ç…§ç‰‡æœ‰äº†â€œæ˜¨æ—¥é‡ç°â€çš„æ•ˆæœï¼Œå¾ˆé€‚åˆå‘æœ‹å‹åœˆã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬åˆ©ç”¨Computer Visionè¯¾å ‚ä¸Šå­¦åˆ°çš„çŸ¥è¯†ï¼Œåœ¨ä¸€äº›æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç…§ç‰‡ä¸Šå®ç°â€œA look into the pastâ€ã€‚
2. åˆ›æ„æè¿°ï¼šæ‰“å¼€æ‰‹æœºç›¸å†Œï¼Œæ‰¾ä¸€äº›æœ‰æ„æ€çš„ç…§ç‰‡ï¼Œå¯¹æŸä¸ªéƒ¨ä½è¿›è¡Œæˆªå–ã€‚å½“ç„¶ï¼Œå¦‚æœä¼šç”¨PSçš„è¯ï¼Œå¯ä»¥å¯¹è¯¥éƒ¨åˆ†åšæ›´ç²¾ç»†åŒ–çš„æˆªå–ç”šè‡³åšä¸€äº›ç‰¹æ•ˆï¼Œå¯ä»¥å¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚å› ä¸ºæ‡’ï¼Œæˆ‘ä»¬å°±ç›´æ¥æˆªå–äº†å›¾ç‰‡æŸä¸ªéƒ¨åˆ†ï¼Œå°†å…¶è½¬åŒ–ä¸ºç°åº¦å›¾åƒï¼Œä¹‹åé€šè¿‡è®¡ç®—æœºè§†è§‰ç®—æ³•å¯¹ä¸¤å¼ å›¾è¿›è¡Œå¤„ç†ï¼Œæœ€ç»ˆå¾—åˆ°â€œA look into the pastâ€çš„æ•ˆæœã€‚
3. è®¡ç®—æœºè§†è§‰é—®é¢˜ï¼šä¸Šè¿°åˆ›æ„çš„å®ç°ï¼Œå¯ä»¥è½¬åŒ–ä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒæ‹¼æ¥é—®é¢˜ï¼Œå…³é”®æŠ€æœ¯æ˜¯å…³é”®ç‚¹æ£€æµ‹ã€åŒ¹é…ä»¥åŠå›¾åƒçš„ç¼åˆã€‚

## åŸç†ä¸å®ç°

OpenCV(å¼€æºè®¡ç®—æœºè§†è§‰åº“)æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰åº“ã€‚è¿™ä¸ªåº“æ˜¯è·¨å¹³å°çš„ï¼Œåœ¨å¼€æºçš„Apache 2è®¸å¯ä¸‹å¯ä»¥å…è´¹ä½¿ç”¨ã€‚OpenCVæ˜¯ç”¨c++ç¼–å†™çš„ï¼Œå®ƒçš„ä¸»è¦æ¥å£æ˜¯ç”¨c++ç¼–å†™çš„ï¼Œä½†å®ƒä¹Ÿæä¾›äº†ä¸€ä¸ªPythonæ¥å£ã€‚æˆ‘ä»¬ä¾é å®ƒæ¥æ£€æµ‹å…³é”®ç‚¹ã€åŒ¹é…å…³é”®ç‚¹å’Œè½¬æ¢å›¾åƒã€‚

### å…³é”®ç‚¹æ£€æµ‹çš„ç®€è¦å†å²
OpenCV-Python^[[Feature Detection and Description -- OpenCV-Python Tutorials beta documentation](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html)]æ•™ç¨‹å¯¹è¿™æ®µç®€çŸ­çš„å†å²å’Œå›é¡¾è¿›è¡Œäº†æ€»ç»“ã€‚

1. Harris Corner Detection:^[Harris, C., & Stephens, M. (1988, August). A combined corner and edge detector. In Alvey vision conference (Vol. 15, No. 50, pp. 10-5244).]

    One early attempt to find these corners was done by Chris Harris & Mike Stephens in their paper A Combined Corner and Edge Detector in 1988, so now it is called Harris Corner Detector.

2. SIFT (Scale-Invariant Feature Transform):^[Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.]

    Harris corner detector is not good enough when scale of image changes. D.Lowe developed a breakthrough method to find scale-invariant features and it is called SIFT in 2004.

3. SURF (Speeded-Up Robust Features):^[Bay, H., Tuytelaars, T., & Van Gool, L. (2006, May). Surf: Speeded up robust features. In European conference on computer vision (pp. 404-417). Springer, Berlin, Heidelberg.]

    SIFT is really good, but not fast enough. In 2006, three people, Bay, H., Tuytelaars, T. and Van Gool, L, introduced a new algorithm called SURF. As name suggests, it is a speeded-up version of SIFT.

4. ORB (Oriented FAST and Rotated BRIEF):^[Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011, November). ORB: An efficient alternative to SIFT or SURF. In 2011 International conference on computer vision (pp. 2564-2571). IEEE.]

    This algorithm was brought up by Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary R. Bradski in their paper ORB: An efficient alternative to SIFT or SURF in 2011. As the title says, it is a good alternative to SIFT and SURF in computation cost, matching performance and mainly the patents. Yes, SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not !!! (correction: the patent of SIFT has already expired now).

ç»¼ä¸Šæ‰€è¿°ï¼ŒHarrisæ˜¯æœ€æ—©åŒ…å«è§’ç‚¹æ£€æµ‹åŸºæœ¬æ€æƒ³çš„äººã€‚SIFTæ˜¯ç¬¬ä¸€ä¸ªæˆç†Ÿçš„ï¼Œä½†æ¯”è¾ƒæ…¢ã€‚SURFæ˜¯SIFTçš„åŠ é€Ÿç‰ˆã€‚ORBæ˜¯SIFTå’ŒSURFçš„å…è´¹é€‰æ‹©

ç”±äºSIFTçš„ä¸“åˆ©å·²äº2020å¹´3æœˆåˆ°æœŸï¼Œæ‰€ä»¥SIFTç°åœ¨å¯ä»¥å…è´¹ä½¿ç”¨ âœŒï¸! ä½†æ˜¯SURFçš„ä¸“åˆ©ç°åœ¨ä»ç„¶æœ‰æ•ˆ ğŸ™.

### åŸºäºSIFTçš„å›¾åƒæ‹¼æ¥
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ç»å…¸çš„SIFTæ£€æµ‹å™¨æ£€æµ‹ä¸¤å¹…å›¾åƒä¸­çš„å…³é”®ç‚¹ï¼Œç„¶ååŒ¹é…è¿™äº›å…³é”®ç‚¹å¹¶æ‹¼æ¥ä¸¤å¹…å›¾åƒã€‚OpenCVä¸­çš„å…¶ä»–å…³é”®ç‚¹æ£€æµ‹å™¨ä¹Ÿæœ‰ç±»ä¼¼SIFTçš„APIï¼Œä¸€æ—¦ä½ çŸ¥é“å¦‚ä½•ä½¿ç”¨SIFTï¼Œå°±å¾ˆå®¹æ˜“ä½¿ç”¨å®ƒä»¬ã€‚æ— è®ºä½¿ç”¨å“ªç§ç®—æ³•ï¼Œæ•´ä¸ªè¿‡ç¨‹éƒ½åŒ…å«ä»¥ä¸‹æ­¥éª¤ã€‚
1. æ£€æµ‹å…³é”®ç‚¹å¹¶ç”Ÿæˆå…³é”®ç‚¹æè¿°ç¬¦
2. åŒ¹é…ä¸¤å¹…å›¾åƒä¹‹é—´æ£€æµ‹åˆ°çš„å…³é”®ç‚¹
3. å¯¹é½ä¸¤ä¸ªå›¾åƒï¼Œå¹¶å°†å®ƒä»¬æ‹¼æ¥æˆä¸€å¹…å›¾

#### æ£€æµ‹å…³é”®ç‚¹å¹¶ç”Ÿæˆæè¿°ç¬¦
é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„å›¾åƒæ˜¯`np.uint8'ç±»å‹ï¼Œç„¶åå°†å®ƒè½¬æ¢ä¸ºç°åº¦ã€‚å› ä¸ºOpenCVä¸­æ‰€æœ‰çš„å…³é”®ç‚¹æ£€æµ‹å™¨åªèƒ½å¤„ç†å•é€šé“å›¾åƒã€‚å¯¹äºå½©è‰²å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ£€æµ‹å‰å°†å…¶è½¬æ¢ä¸ºç°åº¦å›¾åƒï¼Œæˆ–è€…åˆ†åˆ«åœ¨ä¸‰ä¸ªé€šé“ä¸Šè¿›è¡Œæ£€æµ‹ã€‚
```python
img = imread('image/md/left.jpg')

img = np.uint8(img) # make sure it's np.uint8

img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # to Gray Scale
```

<table>
    <tr>
        <td ><center><img src="./image/now.jpg" >Figure 1: now</center></td>
        <td ><center><img src="./image/past_gray.jpg"  >Figure 2: past</center></td>
    </tr>
</table>

ç„¶åå¯¹ä¸¤å¹…å›¾åƒä¸­çš„å…³é”®ç‚¹è¿›è¡Œæ£€æµ‹ï¼Œå¹¶é€šè¿‡SIFTç”Ÿæˆå…³é”®ç‚¹æè¿°ç¬¦ã€‚

```python
# SIFT
sift = cv2.SIFT_create()
keypoints, descriptors = sift.detectAndCompute(img_gray, mask=None)
```

å‡è®¾æ£€æµ‹åˆ°Nä¸ªå…³é”®ç‚¹ï¼Œåˆ™è¿”å›å€¼å…·æœ‰å¦‚ä¸‹ç»“æ„ï¼š

- `keypoints`æ˜¯åŒ…å«Nä¸ª`cv2.KeyPoint`å¯¹è±¡çš„åˆ—è¡¨ã€‚æ¯ä¸ªå…³é”®ç‚¹æœ‰ä»¥ä¸‹å±æ€§ï¼š
  - `angle`ï¼šæè¿°ç¬¦çš„æ–¹å‘
  - `pt`ï¼šä»¥å…ƒç»„`(x,y)`å½¢å¼è¡¨ç¤ºçš„å…³é”®ç‚¹çš„ä½ç½®
  - `response`ï¼šå…³é”®ç‚¹çš„å“åº”å€¼ã€‚è¶Šé«˜ï¼Œè¶Šæœ‰å¯èƒ½æ˜¯ä¸€ä¸ªå…³é”®ç‚¹ã€‚å¯¹äºSIFTï¼Œè¿™æ˜¯DoGå“åº”
  - `size`ï¼šå…³é”®ç‚¹çš„è§„æ¨¡

```python
>>> from pprint import pprint
>>> type(keypoints)
list
>>> p = keypoints[0]
>>> pprint({name: p.__getattribute__(name) for name in dir(p) if not name.startswith('__')})
# You shall see something like this
{'angle': 83.27447509765625,
 ...,
 'pt': (2.505418539047241, 1013.8984375),
 'response': 0.01711214892566204,
 'size': 2.132431745529175}
```

`descriptors`æ˜¯å¤§å°ä¸º`(N,128)`çš„`np.array`ï¼Œæ¯è¡Œå­˜å‚¨å¯¹åº”å…³é”®ç‚¹çš„128ç»´æè¿°ç¬¦ã€‚

```python
>>> descriptors
array([[  3.,   9.,  17., ...,   4.,   2.,   4.],
       [ 39.,   5.,   7., ...,   0.,   1.,   6.],
       [  0.,   0.,   0., ...,  15.,  12.,  11.],
       ...,
       [ 30.,  52.,   4., ...,   0.,   2.,  13.],
       [  0.,   0.,   0., ...,   4.,   2., 136.],
       [ 50., 131.,  30., ...,   0.,   0.,   0.]], dtype=float32)
```

å¯ä»¥é€šè¿‡`cv2.drawKeypoints`å‡½æ•°åœ¨å›¾ä¸Šç”»å‡ºå…³é”®ç‚¹ã€‚`cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS`æ ‡å¿—å‘Šè¯‰å‡½æ•°ä¸ä»…æ˜¾ç¤ºå…³é”®ç‚¹çš„ä½ç½®ï¼Œè€Œä¸”è¿˜æ˜¾ç¤ºå…³é”®ç‚¹çš„å¤§å°å’Œæ–¹å‘ã€‚

```python
# draw keypoints
img_keypoints = cv2.drawKeypoints(
        image     = img,
        keypoints = keypoints,
        outImage  = None,
        flags     = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

write_and_show('img_keypoints.jpg', img_keypoints)
```

<table>
    <tr>
        <td ><center><img src="./result/now_keypoints.jpg" >Figure 3: now keypoints </center></td>
        <td ><center><img src="./result/past_keypoints.jpg"  >Figure 4: past keypoints</center></td>
    </tr>
</table>

#### å…³é”®ç‚¹åŒ¹é…

å‡è®¾æˆ‘ä»¬å·²ç»åœ¨å›¾åƒ1å’Œå›¾åƒ2ä¸­æ£€æµ‹åˆ°å…³é”®ç‚¹ï¼Œå¹¶åƒè¿™æ ·ç”Ÿæˆå®ƒä»¬çš„æè¿°ç¬¦ï¼š

```python
sift = cv2.SIFT_create()
keypoints1, descriptors1 = sift.detectAndCompute(img1_gray, None)
keypoints2, descriptors2 = sift.detectAndCompute(img2_gray, None)
```

ä¸‹ä¸€æ­¥æ˜¯åŒ¹é…ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å…³é”®ç‚¹ã€‚è¿™æ˜¯é€šè¿‡ä»ä¸¤ä¸ªå…·æœ‰ç›¸ä¼¼æè¿°ç¬¦çš„å›¾åƒä¸­æ‰¾åˆ°å…³é”®ç‚¹å¯¹æ¥å®Œæˆçš„ã€‚æè¿°ç¬¦æè¿°å…³é”®ç‚¹å‘¨å›´çš„åŒºåŸŸã€‚ç›¸ä¼¼çš„æè¿°ç¬¦è¡¨ç¤ºç›¸ä¼¼çš„æ¨¡å¼ã€‚æè¿°ç¬¦çš„ç›¸ä¼¼æ€§æ˜¯ç”±å®ƒä»¬çš„æ¬§å‡ é‡Œå¾—è·ç¦»æ¥åº¦é‡çš„ã€‚

##### Brute-force matcher

åŒ¹é…å¯ä»¥é€šè¿‡`cv2.NFMatcher`å®ç°ï¼š

```python
# create matcher
matcher = cv2.BFMatcher_create(crossCheck=True)

# get match
match = matcher.match(
            queryDescriptors = descriptors1,    # query
            trainDescriptors = descriptors2)    # train
# Docstring:
# match(queryDescriptors, trainDescriptors[, mask]) -> matches
# .   @brief Finds the best match for each descriptor from a query set.
```

è¿”å›çš„`match`æ˜¯ä¸€ä¸ª`cv2.DMatch`å¯¹è±¡çš„åˆ—è¡¨ï¼Œæœ‰ä»¥ä¸‹å±æ€§ï¼š

- `distance`ï¼šä¸¤ä¸ªåŒ¹é…å…³é”®ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾·è·ç¦»

- `queryIdx`ï¼šå›¾åƒ1ä¸­åŒ¹é…å…³é”®ç‚¹çš„ç´¢å¼•

- `trainIdx`ï¼šå›¾åƒ2ä¸­åŒ¹é…å…³é”®ç‚¹çš„ç´¢å¼•

```python
>>> type(match)
list
>>> m = match[0]
>>> pprint({name: m.__getattribute__(name) for name in dir(m) if not name.startswith('__')})
{'distance': 236.065673828125,
 ...,
 'queryIdx': 1,
 'trainIdx': 17140}
```

##### åŸºäºFLANNçš„åŒ¹é…å™¨

â€œBFMatcherâ€ä»£è¡¨â€œBrute-Forch Matcherâ€ã€‚è›®åŠ›åŒ¹é…å™¨å¾ˆç®€å•ã€‚å®ƒä½¿ç”¨ä¸€äº›è·ç¦»è®¡ç®—å°†ç¬¬ä¸€ä¸ªé›†åˆä¸­çš„æè¿°ç¬¦ä¸ç¬¬äºŒä¸ªé›†åˆä¸­çš„æ‰€æœ‰å…¶ä»–ç‰¹å¾åŒ¹é…èµ·æ¥ã€‚ç¦»ä½ æœ€è¿‘çš„å°±ä¼šè¿”å›ã€‚

ç„¶è€Œï¼ŒBFMatcherè¶…çº§æ…¢ã€‚FLANNæ˜¯BFMatcherçš„å¿«é€Ÿæ›¿ä»£å“ã€‚FLANNä»£è¡¨â€œè¿‘ä¼¼è¿‘é‚»å¿«é€Ÿå›¾ä¹¦é¦†â€ã€‚å®ƒçš„ç”¨æ³•ç±»ä¼¼äºBFMatcherï¼Œä½†å¯¹äºå¤§å‹æ•°æ®é›†æ¥è¯´å·¥ä½œå¾—æ›´å¿«ã€‚

```python
# create macher
matcher = cv2.FlannBasedMatcher_create()

# get match
match = matcher.match(
            queryDescriptors = descriptors1,    # query
            trainDescriptors = descriptors2)    # train
```

##### åŠ³æ°æ¯”å€¼åˆ¤åˆ«æ³•

æœ‰æ—¶åŒ¹é…ç»“æœåŒ…å«å¾ˆå¤šé”™è¯¯çš„åŒ¹é…ã€‚æˆ‘ä»¬å¯ä»¥åƒLoweçš„è®ºæ–‡é‚£æ ·é€šè¿‡æ¯”ç‡æ£€éªŒå»æ‰ä¸€éƒ¨åˆ†ã€‚Loweâ€™s ratio testçš„åŸºæœ¬æ€æƒ³æ˜¯:ç¬¬ä¸€å¹…å›¾åƒä¸­çš„æ¯ä¸ªå…³é”®ç‚¹ä¸ç¬¬äºŒå¹…å›¾åƒä¸­çš„å¤šä¸ªå…³é”®ç‚¹è¿›è¡ŒåŒ¹é…ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªå…³é”®ç‚¹ä¿ç•™2ä¸ªæœ€ä½³åŒ¹é…(æœ€ä½³åŒ¹é…=è·ç¦»åº¦é‡æœ€å°çš„åŒ¹é…)ã€‚åŠ³çš„è¯•éªŒæ£€éªŒäº†è¿™ä¸¤ä¸ªè·ç¦»æ˜¯å¦è¶³å¤Ÿä¸åŒã€‚å¦‚æœä¸æ˜¯ï¼Œåˆ™è¯¥å…³é”®ç‚¹å°†è¢«æ¶ˆé™¤ï¼Œä¸å†ç”¨äºè¿›ä¸€æ­¥çš„è®¡ç®—ã€‚

```python
matcher = cv2.FlannBasedMatcher_create()

# get best two matches
best_2 = matcher.knnMatch(
            queryDescriptors = descriptors1,
            trainDescriptors = descriptors2,
            k                = 2)

# Lowe's ratio test
ratio = 0.7
match = []
for m,n in best_2:
    if m.distance < ratio*n.distance:
        match.append(m)
```

##### é€‰æ‹©è‰¯å¥½çš„åŒ¹é…

`distance`è¡¡é‡åŒ¹é…çš„å¥½åç¨‹åº¦ï¼Œæˆ‘ä»¬åªé€‰æ‹©äº†å…·æœ‰æœ€å°`distance`çš„åŒ¹é…ï¼Œå»é™¤äº†é‚£äº›æ›´å¤§è·ç¦»çš„åŒ¹é…ã€‚

```python
# sort by distance
match = sorted(match, key = lambda x:x.distance)

# take the best 100 matches
match = match[:100]
```

##### åŒ¹é…å¯è§†åŒ–

æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡½æ•°`cv2.drawMatches`å¯è§†åŒ–æ‰€æœ‰çš„åŒ¹é…å…³é”®ç‚¹ã€‚

```python
match_draw = cv2.drawMatches(
        img1        = img1,
        keypoints1  = keypoints1,
        img2        = img2,
        keypoints2  = keypoints2,
        matches1to2 = match,
        outImg      = None,
        flags       = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
```

<table>
    <tr>
        <td ><center><img src="./result/match.jpg" >Figure 5: visualize match keypoints</center></td>
    </tr>
</table>

#### å›¾åƒæ‹¼æ¥

æœ€åä¸€æ­¥æ˜¯å°†å®ƒä»¬æ‹¼æ¥æˆä¸€ä¸ªå¤§å›¾åƒã€‚é¦–å…ˆï¼Œå¾—åˆ°æ‰€æœ‰åŒ¹é…å…³é”®ç‚¹çš„åæ ‡ï¼š

```python
# get coordinates of matched pairs
keypoints1 = np.array([keypoints1[m.queryIdx].pt for m in match])
keypoints2 = np.array([keypoints2[m.trainIdx].pt for m in match])
```

##### é€è§†å˜æ¢

ç„¶åæˆ‘ä»¬éœ€è¦å˜æ¢å›¾åƒ2ï¼Œä½¿å…¶å…³é”®ç‚¹ä¸å›¾åƒ1ä¸­çš„å…³é”®ç‚¹ç›¸åŒ¹é…ã€‚è¿™æ˜¯é€šè¿‡ä»åŒ¹é…çš„å…³é”®ç‚¹è®¡ç®—ä¸€ä¸ªé€è§†å˜æ¢ï¼Œç„¶åå°†å˜æ¢åº”ç”¨åˆ°å›¾åƒ2æ¥å®Œæˆçš„ã€‚

æ­¤åï¼Œæˆ‘ä»¬å°†å›¾åƒ2ç§°ä¸ºæºå›¾åƒï¼Œå›¾åƒ1ç§°ä¸ºç›®æ ‡å›¾åƒã€‚è®¡ç®—ä»æºå›¾åƒåˆ°ç›®æ ‡å›¾åƒçš„é€è§†å˜æ¢ï¼š

```python
src, dst = img2, img1
src_kps, dst_kps = (keypoints2, keypoints1)

T, status = cv2.findHomography(
                    srcPoints = src_kps,
                    dstPoints = dst_kps,
                    method    = cv2.USAC_ACCURATE,
                    ransacReprojThreshold = 3)
```

å¹¶éæ‰€æœ‰åŒ¹é…çš„å…³é”®ç‚¹å¯¹éƒ½æ˜¯æ­£ç¡®çš„ã€‚ä¸æ­£ç¡®çš„åŒ¹é…ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„è½¬æ¢ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥å˜æ¢åçš„å¯¹æ˜¯å¦è¶³å¤Ÿæ¥è¿‘æ¥åˆ¤æ–­åŒ¹é…æ˜¯å¦æ­£ç¡®ï¼Œè¿™å¯ä»¥é€šè¿‡`cv2.USAC_ACCURATE`æ¥æ‰§è¡Œã€‚`ransacReprojThreshold`å‚æ•°æ˜¯å°†ä¸€ä¸ªç‚¹å¯¹è§†ä¸ºæ­£ç¡®çš„æœ€å¤§å…è®¸é‡æŠ•å½±è¯¯å·®ã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œå…è®¸çš„æœ€å¤§é‡æŠ•å½±è¯¯å·®æ˜¯3åƒç´ ã€‚

è¿”å›å€¼`status`è¡¨ç¤ºå…³é”®ç‚¹çš„æ­£ç¡®æ€§ã€‚`status[i]==1`è¡¨ç¤º`src_kps[i]`å’Œ`dst_kps[i]`æ˜¯æ­£ç¡®çš„å¯¹ã€‚

è¿”å›å€¼`T`æ˜¯ä¸€ä¸ª$3\times3$çš„å˜æ¢çŸ©é˜µ $$\begin{equation} T = \begin{bmatrix} h_{11} & h_{12} & h_{13} \ h_{21} & h_{22} & h_{23} \ h_{31} & h_{32} & h_{33} \ \end{bmatrix}, \end{equation}$$_

å°†ä¸€ä¸ªç‚¹ä» $(x,y)$å˜æ¢åˆ°ä½ç½®$(x', y')$ï¼š

_$$\begin{equation} \left{ \begin{matrix} x' =\dfrac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}\ y' =\dfrac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}} \end{matrix} \right. \end{equation}$$

æˆ‘ä»¬å¯ä»¥é€šè¿‡`cv2.warpPerspective`å¯¹å›¾åƒ2åº”ç”¨å˜æ¢`T`ã€‚

```python
H, W, _ = img2.shape

new_img2 = cv2.warpPerspective(
                    src   = img2,
                    M     = T,
                    dsize = (W, H),
                    dst   = np.zeros_like(img),
                    borderMode = cv2.BORDER_TRANSPARENT)
```

`dsize`æŒ‡å®šè½¬æ¢åçš„å›¾åƒçš„å¤§å°ã€‚

æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½çš„å®ç°â€pastâ€œæ•ˆæœï¼Œæˆ‘ä»¬å°†å›¾ç‰‡1ä¸­å›¾ç‰‡2æ‰€å ä½ç½®çš„åƒç´ å…¨éƒ¨ç½®ä¸º0ã€‚

```python
index = (new_img2!=0)
img1[index] = 0
```

<table>
    <tr>
        <td ><center><img src="./result/past_transformed.jpg" >Figure 6: perspective transformation to past</center></td>
        <td ><center><img src="./result/now_transformed.jpg" >Figure 7: perspective transformation to now</center></td>
    </tr>
</table>

##### å›¾åƒå åŠ 

æœ€åä¸€æ­¥æ˜¯æŠŠå®ƒä»¬å †åœ¨ä¸€èµ·ã€‚ç›´æ¥å¹³å‡ç»™å‡ºä»¥ä¸‹ç»“æœã€‚

```python
direct_mean = new_img1/2 + new_img2/2
imshow('direct_mean.jpg', direct_mean)
```

å®é™…ä¸Šï¼Œæˆ‘ä»¬åªéœ€è¦å–é‡å éƒ¨åˆ†çš„å¹³å‡å€¼ã€‚å¯¹äºæœªé‡å çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬åº”è¯¥å¤åˆ¶å›¾åƒ1æˆ–å›¾åƒ2çš„åƒç´ å€¼ã€‚è¿™å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥å®ç°ï¼š

```python
# smart average
cnt = np.zeros([H,W,1]) + 1e-10     # add a tiny value to avoid ZeroDivisionError
cnt += (new_img2 != 0).any(2, keepdims=True)
cnt += (new_img1 != 0).any(2, keepdims=True)

# convert to floating number to avoid overflow
new_img1 = np.float32(new_img1)
new_img2 = np.float32(new_img2)

stack = (new_img2+new_img1)/cnt
imshow('stack.jpg', stack)
```

`cnt`ç»Ÿè®¡åœ¨`(i,j)`å¤„æœ‰å¤šå°‘ä¸ªæœ‰æ•ˆåƒç´ çš„å›¾åƒï¼š

- å¯¹äºé‡å éƒ¨åˆ†ï¼Œ`cnt[i,j]` ç­‰äº2
- å¦‚æœåªæœ‰ä¸€å¹…å›¾åƒåœ¨`(i,j)`å¤„æœ‰æœ‰æ•ˆåƒç´ ï¼Œ`cnt[i,j]`ç­‰äº1
- å¦‚æœæ²¡æœ‰å›¾åƒåœ¨`(i,j)`å¤„æœ‰æœ‰æ•ˆåƒç´ ï¼Œåˆ™`cnt[i,j]`ä¸º0

## æ•ˆæœå±•ç¤º

åœ¨å›¾åƒæ‹¼æ¥è¿™å—ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯å·²ç»å®ç°å¾—éå¸¸å¥½äº†ã€‚å› ä¸ºæ‡’å¾—æŠ å›¾ï¼Œæˆ‘ä»¬åªæ˜¯éšæ„çš„å°†å›¾ç‰‡ä¸­æŸéƒ¨åˆ†æˆªå–ä¸‹æ¥è¿›è¡Œç®€å•çš„â€œpastâ€å¤„ç†ï¼Œä¹Ÿå°±æ˜¯è½¬ä¸ºé»‘ç™½ç…§ç‰‡ï¼ˆç°åº¦å›¾åƒï¼‰ã€‚æ„¿æ„èŠ±æ—¶é—´å¤„ç†å›¾ç‰‡çš„è¯ï¼Œå¯ä»¥å°†â€œA look into the pastâ€æ¸²æŸ“å¾—å¾ˆå¥½çœ‹ï¼Œè¿™é‡Œç®€å•å±•ç¤ºä¸Šé¢çš„ç¨‹åºå‘˜ç‰ˆæ—ºä»”ç‰›å¥¶ï¼š

<table>
    <tr>
        <td ><center><img src="./result/stack.jpg" >Figure 8: stack image</center></td>
    </tr>
</table>



## å·¥ç¨‹ç»“æ„

```
.
â”œâ”€â”€ image
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ result
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”œâ”€â”€ sift.py
â”‚Â Â  â””â”€â”€ utils.py
â””â”€â”€ test.py
```

## è¿è¡Œè¯´æ˜

æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„pythonåº“ä»¥åŠç‰ˆæœ¬å¦‚ä¸‹ï¼š

```requirements.txt
numpy==1.21.5
opencv-python==4.5.4.60
tqdm==4.62.3
```

æŒ‰ç…§å¦‚ä¸‹å‘½ä»¤è¿è¡Œï¼š

```shell
pip install -r requirements.txt
cd src/ && python main.py
```

