<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="LI, Qimai" />
  <meta name="dcterms.date" content="October 11, 2021" />
  <title>Keypoints Detection and Match</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="image/github-pandoc.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Keypoints Detection and Match</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#install-opencv"><span class="toc-section-number">1</span> Install OpenCV</a>
<ul>
<li><a href="#on-windows"><span class="toc-section-number">1.1</span> On Windows</a></li>
<li><a href="#on-macos"><span class="toc-section-number">1.2</span> On macOS</a></li>
</ul></li>
<li><a href="#readsaveshow-images-in-opencv"><span class="toc-section-number">2</span> Read/save/show images in OpenCV</a>
<ul>
<li><a href="#read-images"><span class="toc-section-number">2.1</span> Read images</a></li>
<li><a href="#save-images"><span class="toc-section-number">2.2</span> Save images</a></li>
<li><a href="#show-images"><span class="toc-section-number">2.3</span> Show images</a></li>
<li><a href="#some-utilities"><span class="toc-section-number">2.4</span> Some utilities</a></li>
</ul></li>
<li><a href="#a-brief-history-of-keypoints-detection"><span class="toc-section-number">3</span> A brief history of keypoints detection</a></li>
<li><a href="#image-alignment-via-sift"><span class="toc-section-number">4</span> Image alignment via SIFT</a>
<ul>
<li><a href="#detect-keypoints-and-generate-descriptors"><span class="toc-section-number">4.1</span> Detect keypoints and generate descriptors</a></li>
<li><a href="#match"><span class="toc-section-number">4.2</span> Match</a></li>
<li><a href="#stich-images"><span class="toc-section-number">4.3</span> Stich images</a></li>
</ul></li>
<li><a href="#panorama-stitching"><span class="toc-section-number">5</span> Panorama stitching</a></li>
<li><a href="#video-stabilization-and-stack-denoising"><span class="toc-section-number">6</span> Video stabilization and stack denoising</a>
<ul>
<li><a href="#long-exposure"><span class="toc-section-number">6.1</span> Long exposure</a></li>
<li><a href="#deraining"><span class="toc-section-number">6.2</span> Deraining</a></li>
</ul></li>
<li><a href="#assignment-12-points-2-bonus-points"><span class="toc-section-number">7</span> Assignment (12 points + 2 bonus points)</a>
<ul>
<li><a href="#image-alignment-via-sift-9-points"><span class="toc-section-number">7.1</span> Image alignment via SIFT (9 points)</a></li>
<li><a href="#panorama-stitching-3-points"><span class="toc-section-number">7.2</span> Panorama stitching (3 points)</a></li>
<li><a href="#video-stabilization-and-deraining-2-bonus-points"><span class="toc-section-number">7.3</span> Video stabilization and deraining (2 bonus points)</a></li>
<li><a href="#submission-instruction"><span class="toc-section-number">7.4</span> Submission instruction</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<h1 data-number="1" id="install-opencv"><span class="header-section-number">1</span> Install OpenCV</h1>
<p>OpenCV (Open Source Computer Vision Library) is a powerful library designed for computer vision. The library is cross-platform and free for use under the open-source Apache 2 License. OpenCV is written in C++ and its primary interface is in C++, but it also provides a Python interface. In this tutorial, we rely on it to detect keypoints, match them, and transform images.</p>
<h2 data-number="1.1" id="on-windows"><span class="header-section-number">1.1</span> On Windows</h2>
Please open your anaconda propmt from Start menu.
<center>
<img src="image/md/conda_prompt.jpg" style="width: 50%"/>
</center>
<p>Create a new python environment with <code>opencv-python 4.5</code>, <code>tqdm</code>, and <code>ipython</code> by following command.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">base</span><span class="kw">)</span> <span class="ex">C:\Users\%USERNAME%</span><span class="op">&gt;</span> conda create <span class="at">--name</span> opencv <span class="at">-c</span> conda-forge opencv=4.5 tqdm ipython</span></code></pre></div>
<p>Test your installation and you shall see the version of OpenCV is <code>4.5.x</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">base</span><span class="kw">)</span> <span class="ex">C:\Users\%USERNAME%</span><span class="op">&gt;</span> conda activate opencv</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">opencv</span><span class="kw">)</span> <span class="ex">C:\Users\%USERNAME%</span><span class="op">&gt;</span> python <span class="at">-c</span> <span class="st">&quot;import cv2; print(cv2.__version__)&quot;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">4.5.3</span></span></code></pre></div>
<p>Every time you open anaconda propmt, you need to activate this environment before using OpenCV.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">base</span><span class="kw">)</span> <span class="ex">C:\Users\%USERNAME%</span><span class="op">&gt;</span> conda activate opencv</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="ex">opencv</span><span class="kw">)</span> <span class="ex">C:\Users\%USERNAME%</span><span class="op">&gt;</span> conda activate opencv</span></code></pre></div>
<p>The newly created environment is located at <code>C:\Users\%USERNAME%\anaconda3\envs\opencv</code>. If you are using PyCharm, do not forget to set your project interpreter as <code>SetupPython.html</code> did in last tutorial.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set project interpreter of PyCharm to</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">C:\Users\%USERNAME%\anaconda3\envs\opencv\python.exe</span></span></code></pre></div>
<h2 data-number="1.2" id="on-macos"><span class="header-section-number">1.2</span> On macOS</h2>
<p>Open your Terminal app and create a new python environment with <code>opencv-python 4.5</code>, <code>tqdm</code>, and <code>ipython</code> by following command.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> conda create <span class="at">--name</span> opencv <span class="at">-c</span> conda-forge opencv=4.5 tqdm ipython</span></code></pre></div>
<p>Test your installation and you shall see the version of OpenCV is <code>4.5.x</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> conda activate opencv</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> python <span class="at">-c</span> <span class="st">&quot;import cv2; print(cv2.__version__)&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">4.5.3</span></span></code></pre></div>
<p>Every time you open anaconda propmt, you need to activate this environment before using OpenCV.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> conda activate opencv</span></code></pre></div>
<p>The newly created environment is located at <code>/Users/USERNAME/miniconda3/envs/opencv</code>. If you are using PyCharm, do not forget to set your project interpreter as <code>SetupPython.html</code> did in last tutorial.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set project interpreter of PyCharm to</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">/Users/USERNAME/miniconda3/envs/opencv/bin/python</span></span></code></pre></div>
<h1 data-number="2" id="readsaveshow-images-in-opencv"><span class="header-section-number">2</span> Read/save/show images in OpenCV</h1>
<p>OpenCV provides a set of APIs for us to easily read/write/show images.</p>
<h2 data-number="2.1" id="read-images"><span class="header-section-number">2.1</span> Read images</h2>
<p><code>cv2.imread</code> reads in an image as <code>np.ndarray</code> object with <code>dtype==np.uint8</code>, i.e., 8-bit unsigned integer.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imread(<span class="st">&#39;image/md/left.jpg&#39;</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(img)   <span class="co"># np.ndarray</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>img.shape   <span class="co"># (1512, 2016, 3), which indicates a color image.</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>img.dtype   <span class="co"># dtype(&#39;uint8&#39;)</span></span></code></pre></div>
<p>You had better convert it to floating types before further processing to avoid any possible overflow.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> np.float32(img)</span></code></pre></div>
<h2 data-number="2.2" id="save-images"><span class="header-section-number">2.2</span> Save images</h2>
<p><code>cv2.imwrite</code> takes an <code>np.ndarray</code> object as image and save it to file. If the <code>dtype</code> of the array is not <code>np.uint8</code>, the function will convert it to <code>np.uint8</code> before saving.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cv2.imwrite(<span class="st">&#39;tmp.jpg&#39;</span>,img) <span class="co"># return True, if write successfully.</span></span></code></pre></div>
<h2 data-number="2.3" id="show-images"><span class="header-section-number">2.3</span> Show images</h2>
<p><code>cv2.imshow</code> shows an image, but the image won’t be displayed util you call <code>cv2.waitKey</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>cv2.imshow(<span class="st">&quot;window title&quot;</span>, img)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>cv2.waitKey(<span class="dv">1</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># do not omit `1`, or else you have to hit a</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># key in OpenCV window to return from waitKey.</span></span></code></pre></div>
<p><code>cv2.waitKey(delay)</code> will wait for <code>delay</code> milliseconds for user to hit a key and return the key user hit or <code>-1</code> if no key is hit. Its side-effect is to update OpenCV windows. If it is invoked without <code>delay</code> parameter or <code>deplay=0</code>, it will wait for infinite time. If you just want to update OpenCV window display and not really want user to hit a key, please use <code>cv2.waitKey(1)</code> like above.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># wait for infinite time</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>cv2.waitKey()</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>cv2.waitKey(<span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># wait for just 1 ms, but can update window display</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>cv2.waitKey(<span class="dv">1</span>)</span></code></pre></div>
<p>Use <code>cv2.destroyAllWindows</code> to close all windows. <code>cv2.waitKey</code> should also be called to let OpenCV update (close) windows.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>cv2.destroyAllWindows()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>cv2.waitKey(<span class="dv">1</span>)</span></code></pre></div>
<h2 data-number="2.4" id="some-utilities"><span class="header-section-number">2.4</span> Some utilities</h2>
<p>Some utility function are provided in <code>utils.py</code> for you to easily read/write/show images.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> imread, imshow, write_and_show, destroyAllWindows</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> imread(<span class="st">&#39;image/md/left.jpg&#39;</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>imshow(<span class="st">&#39;tmp.jpg&#39;</span>, img)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>write_and_show(<span class="st">&#39;tmp.jpg&#39;</span>,img)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># give you some time to view all windows before close</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>destroyAllWindows()  </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># close immediately</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>destroyAllWindows(wait_key<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<h1 data-number="3" id="a-brief-history-of-keypoints-detection"><span class="header-section-number">3</span> A brief history of keypoints detection</h1>
<p>This brief history and review is summarised by OpenCV-Python tutorials<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<ol type="1">
<li><p>Harris Corner Detection:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>One early attempt to find these corners was done by Chris Harris &amp; Mike Stephens in their paper A Combined Corner and Edge Detector in 1988, so now it is called Harris Corner Detector.</p></li>
<li><p>SIFT (Scale-Invariant Feature Transform):<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Harris corner detector is not good enough when scale of image changes. D.Lowe developed a breakthrough method to find scale-invariant features and it is called SIFT in 2004.</p></li>
<li><p>SURF (Speeded-Up Robust Features):<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>SIFT is really good, but not fast enough. In 2006, three people, Bay, H., Tuytelaars, T. and Van Gool, L, introduced a new algorithm called SURF. As name suggests, it is a speeded-up version of SIFT.</p></li>
<li><p>ORB (Oriented FAST and Rotated BRIEF):<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>This algorithm was brought up by Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary R. Bradski in their paper ORB: An efficient alternative to SIFT or SURF in 2011. As the title says, it is a good alternative to SIFT and SURF in computation cost, matching performance and mainly the patents. Yes, SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not !!! (correction: the patent of SIFT has already expired now).</p></li>
</ol>
<p>In summary, Harris is the early one containing basic idea of corner detection. SIFT is the first mature one, but slow. SURF is a speeded-up version of SIFT. ORB is a free alternative for SIFT and SURF.</p>
<p>However, the patent of SIFT expired in March of 2020, so SIFT is free to use now ✌️! But patent of SURF is still valid now 🙁.</p>
<h1 data-number="4" id="image-alignment-via-sift"><span class="header-section-number">4</span> Image alignment via SIFT</h1>
<p>In this section, we are going to detect keypoints in two images via the classic SIFT detector then match these keypoints and align two images. Other keypoint detectors in OpenCV have similar API as SIFT, it is easy to use them once you know how to use SIFT. No matter which algorithm you use, the whole procedure always contains following steps.</p>
<ol type="1">
<li>Detect keypoints and generate keypoint descriptors.</li>
<li>Match detected keypoints between two images.</li>
<li>Align two images and stich them into one.</li>
</ol>
<p>The two example images and final stitching result are shown here.</p>
<center>
<img alt="left.jpg" style="width:40%" src="image/md/left.jpg"/> <img alt="right.jpg" style="width:40%" src="image/md/right.jpg"/> <img alt="right.jpg" style="width:80%" src="image/md/stack.jpg"/>
<p>
Fig 1. (a) image 1. (b) image 2. (c) stitching result.
</p>
</center>
<h2 data-number="4.1" id="detect-keypoints-and-generate-descriptors"><span class="header-section-number">4.1</span> Detect keypoints and generate descriptors</h2>
<p>First, we need to make sure our images are of type <code>np.uint8</code> and then convert it to grayscale. Because all keypoint detector in OpenCV can only deal with single-channel images. To process colourful image, we can either convert them to grayscale image before detection, or perform detection on three channels separately.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> imread(<span class="st">&#39;image/md/left.jpg&#39;</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> np.uint8(img) <span class="co"># make sure it&#39;s np.uint8</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>img_gray <span class="op">=</span> cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) <span class="co"># to Gray Scale</span></span></code></pre></div>
<p>Then we detect keypoints in two images and generate descriptors for them via SIFT.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SIFT</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>sift <span class="op">=</span> cv2.SIFT_create()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>keypoints, descriptors <span class="op">=</span> sift.detectAndCompute(img_gray, mask<span class="op">=</span><span class="va">None</span>)</span></code></pre></div>
<p>Assume <code>N</code> keypoints are detected, then return values have following structure:</p>
<ul>
<li><code>keypoints</code> is a list containing N <code>cv2.KeyPoint</code> objects. Every keypoint has following attributes:
<ul>
<li><code>angle</code>: orientation of the descriptor.</li>
<li><code>pt</code>: location of the keypoint in form of tuple <code>(x,y)</code>.</li>
<li><code>response</code>: keypoint response. the higher, the more likely it is a keypoint. For SIFT, this is the DoG response.</li>
<li><code>size</code>: scale of the keypoint.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">type</span>(keypoints)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> p <span class="op">=</span> keypoints[<span class="dv">0</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pprint({name: p.<span class="fu">__getattribute__</span>(name) <span class="cf">for</span> name <span class="kw">in</span> <span class="bu">dir</span>(p) <span class="cf">if</span> <span class="kw">not</span> name.startswith(<span class="st">&#39;__&#39;</span>)})</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># You shall see something like this</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;angle&#39;</span>: <span class="fl">83.27447509765625</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a> ...,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a> <span class="st">&#39;pt&#39;</span>: (<span class="fl">2.505418539047241</span>, <span class="fl">1013.8984375</span>),</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a> <span class="st">&#39;response&#39;</span>: <span class="fl">0.01711214892566204</span>,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a> <span class="st">&#39;size&#39;</span>: <span class="fl">2.132431745529175</span>}</span></code></pre></div>
<ul>
<li><code>descriptors</code> is an <code>np.array</code> of size <code>(N, 128)</code>. Each row stores an 128-dimensional descriptor for the corresponding keypoint.</li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> descriptors</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>array([[  <span class="fl">3.</span>,   <span class="fl">9.</span>,  <span class="fl">17.</span>, ...,   <span class="fl">4.</span>,   <span class="fl">2.</span>,   <span class="fl">4.</span>],</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>       [ <span class="fl">39.</span>,   <span class="fl">5.</span>,   <span class="fl">7.</span>, ...,   <span class="fl">0.</span>,   <span class="fl">1.</span>,   <span class="fl">6.</span>],</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>       [  <span class="fl">0.</span>,   <span class="fl">0.</span>,   <span class="fl">0.</span>, ...,  <span class="fl">15.</span>,  <span class="fl">12.</span>,  <span class="fl">11.</span>],</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>       ...,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>       [ <span class="fl">30.</span>,  <span class="fl">52.</span>,   <span class="fl">4.</span>, ...,   <span class="fl">0.</span>,   <span class="fl">2.</span>,  <span class="fl">13.</span>],</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>       [  <span class="fl">0.</span>,   <span class="fl">0.</span>,   <span class="fl">0.</span>, ...,   <span class="fl">4.</span>,   <span class="fl">2.</span>, <span class="fl">136.</span>],</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>       [ <span class="fl">50.</span>, <span class="fl">131.</span>,  <span class="fl">30.</span>, ...,   <span class="fl">0.</span>,   <span class="fl">0.</span>,   <span class="fl">0.</span>]], dtype<span class="op">=</span>float32)</span></code></pre></div>
<p>We draw the detected keypoints on images by <code>cv2.drawKeypoints</code> function. Flag <code>cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</code> tells the function to show not only the location but also the size and orientation of keypoints.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># draw keypoints</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>img_keypoints <span class="op">=</span> cv2.drawKeypoints(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        image     <span class="op">=</span> img,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        keypoints <span class="op">=</span> keypoints,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        outImage  <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        flags     <span class="op">=</span> cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>write_and_show(<span class="st">&#39;img_keypoints.jpg&#39;</span>, img_keypoints)</span></code></pre></div>
The detection results of two example images:
<center>
<a href="image/md/left_keypoints.jpg"> <img alt="left keypoints" style="width:45%" src="image/md/left_keypoints.jpg"/> </a>    <a href="image/md/right_keypoints.jpg"> <img alt="right keypoints" style="width:45%" src="image/md/right_keypoints.jpg"/> </a>
<p>
Fig 2. Keypoints detected.
</p>
</center>
<h2 data-number="4.2" id="match"><span class="header-section-number">4.2</span> Match</h2>
<p>Assume we have detected keypoints in both image 1 and image 2 and generate their descriptors like this:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>sift <span class="op">=</span> cv2.SIFT_create()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>keypoints1, descriptors1 <span class="op">=</span> sift.detectAndCompute(img1_gray, <span class="va">None</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>keypoints2, descriptors2 <span class="op">=</span> sift.detectAndCompute(img2_gray, <span class="va">None</span>)</span></code></pre></div>
<p>The next step is to match keypoints between two images. This is done by finding keypoint pairs from two images with similar descriptors. The descriptor describes what the area around a keypoint looks like. Similar descriptors indicate similar patterns. The similarity of descriptors is measured by their euclidean distance. Assume we have two 128-dimensional keypoint descriptors <span class="math inline">\(u,v\in\mathbb{R}^{128}\)</span>, their distance is defined as <span class="math display">\[d(u,v) = \sqrt{\sum_{i=1}^{128}(u_i-v_i)^2}\]</span> Small <span class="math inline">\(d(u,v)\)</span> value indicates keypoint <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> looks similar. For each keypoint in image 1, we always match it with the most similar keypoint from image 2.</p>
<h3 data-number="4.2.1" id="brute-force-matcher"><span class="header-section-number">4.2.1</span> Brute-force matcher</h3>
<p>The matching could be done by <code>cv2.BFMatcher</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create matcher</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>matcher <span class="op">=</span> cv2.BFMatcher_create(crossCheck<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get match</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> matcher.match(</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>            queryDescriptors <span class="op">=</span> descriptors1,    <span class="co"># query</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>            trainDescriptors <span class="op">=</span> descriptors2)    <span class="co"># train</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Docstring:</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># match(queryDescriptors, trainDescriptors[, mask]) -&gt; matches</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># .   @brief Finds the best match for each descriptor from a query set.</span></span></code></pre></div>
<p>The returned <code>match</code> is a list of <code>cv2.DMatch</code> object with following attributes:</p>
<ul>
<li><code>distance</code>: euclidean distance between two matched keypoints as above formula.</li>
<li><code>queryIdx</code>: the index of matched keypoints in <strong>image 1</strong>.</li>
<li><code>trainIdx</code>: the index of matched keypoints in <strong>image 2</strong>.</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">type</span>(match)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> m <span class="op">=</span> match[<span class="dv">0</span>]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> pprint({name: m.<span class="fu">__getattribute__</span>(name) <span class="cf">for</span> name <span class="kw">in</span> <span class="bu">dir</span>(m) <span class="cf">if</span> <span class="kw">not</span> name.startswith(<span class="st">&#39;__&#39;</span>)})</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;distance&#39;</span>: <span class="fl">236.065673828125</span>,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a> ...,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a> <span class="st">&#39;queryIdx&#39;</span>: <span class="dv">1</span>,</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a> <span class="st">&#39;trainIdx&#39;</span>: <span class="dv">17140</span>}</span></code></pre></div>
<h3 data-number="4.2.2" id="flann-based-matcher"><span class="header-section-number">4.2.2</span> FLANN based matcher</h3>
<p>“BFMatcher” stands for “Brute-Forch Matcher”. Brute-Force matcher is simple. It matches the descriptor in first set with all other features in second set using some distance calculation. And the closest one is returned.</p>
<p>However, BFMatcher is super slow. FLANN is an fast alternative to BFMatcher. FLANN stands for “Fast Library for Approximate Nearest Neighbors”. Its usage is similar to BFMatcher but works more faster for large datasets.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create macher</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>matcher <span class="op">=</span> cv2.FlannBasedMatcher_create()</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get match</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> matcher.match(</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>            queryDescriptors <span class="op">=</span> descriptors1,    <span class="co"># query</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            trainDescriptors <span class="op">=</span> descriptors2)    <span class="co"># train</span></span></code></pre></div>
<h3 data-number="4.2.3" id="lowes-ratio-test"><span class="header-section-number">4.2.3</span> Lowe’s ratio test</h3>
<p>Sometimes the matching result contains lots of false matches. We could remove part of them by ratio test as Lowe’s paper. The basic idea of Lowe’s ratio test: each keypoint of the first image is matched with a number of keypoints from the second image. We keep the 2 best matches for each keypoint (best matches = the ones with the smallest distance measurement). Lowe’s test checks that the two distances are sufficiently different. If they are not, then the keypoint is eliminated and will not be used for further calculations.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>matcher <span class="op">=</span> cv2.FlannBasedMatcher_create()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># get best two matches</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>best_2 <span class="op">=</span> matcher.knnMatch(</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>            queryDescriptors <span class="op">=</span> descriptors1,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            trainDescriptors <span class="op">=</span> descriptors2,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            k                <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Lowe&#39;s ratio test</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>ratio <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> []</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m,n <span class="kw">in</span> best_2:</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m.distance <span class="op">&lt;</span> ratio<span class="op">*</span>n.distance:</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        match.append(m)</span></code></pre></div>
<h3 data-number="4.2.4" id="select-good-matches"><span class="header-section-number">4.2.4</span> Select good matches</h3>
<p><code>distance</code> measures goodness of the match. We only select the match with small <code>distance</code>, and remove those with large distance.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sort by distance</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> <span class="bu">sorted</span>(match, key <span class="op">=</span> <span class="kw">lambda</span> x:x.distance)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># take the best 100 matches</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> match[:<span class="dv">100</span>]</span></code></pre></div>
<h3 data-number="4.2.5" id="draw-match"><span class="header-section-number">4.2.5</span> Draw match</h3>
<p>We can visualize all match keypoints by function <code>cv2.drawMatches</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>match_draw <span class="op">=</span> cv2.drawMatches(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>        img1        <span class="op">=</span> img1,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>        keypoints1  <span class="op">=</span> keypoints1,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        img2        <span class="op">=</span> img2,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        keypoints2  <span class="op">=</span> keypoints2,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        matches1to2 <span class="op">=</span> match,</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        outImg      <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        flags       <span class="op">=</span> cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)</span></code></pre></div>
<center>
<a href="image/md/match.jpg"> <img alt="matched keypoints" style="width:90%" src="image/md/match.jpg"/> </a>
<p>
Fig 3. Matched keypoints.
</p>
</center>
<h2 data-number="4.3" id="stich-images"><span class="header-section-number">4.3</span> Stich images</h2>
<p>The final step is to stitch them into a single large image. First, we get coordinates of all matched keypoints:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get coordinates of matched pairs</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>keypoints1 <span class="op">=</span> np.array([keypoints1[m.queryIdx].pt <span class="cf">for</span> m <span class="kw">in</span> match])</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>keypoints2 <span class="op">=</span> np.array([keypoints2[m.trainIdx].pt <span class="cf">for</span> m <span class="kw">in</span> match])</span></code></pre></div>
<h3 data-number="4.3.1" id="perspective-transform"><span class="header-section-number">4.3.1</span> Perspective transform</h3>
<p>Then we need to transform image 2 to align its keypoints with the matches ones in image 1. This is done by calculate a perspective transform from matched keypoints and then apply the transform to image 2.</p>
<p>Hereafter, we refer image 2 as <strong>source image</strong> and image 1 as <strong>destination image</strong>. Calculate a perspective transform from source to destination image:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>src, dst <span class="op">=</span> img2, img1</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>src_kps, dst_kps <span class="op">=</span> (keypoints2, keypoints1)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>T, status <span class="op">=</span> cv2.findHomography(</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>                    srcPoints <span class="op">=</span> src_kps,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>                    dstPoints <span class="op">=</span> dst_kps,</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>                    method    <span class="op">=</span> cv2.USAC_ACCURATE,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>                    ransacReprojThreshold <span class="op">=</span> <span class="dv">3</span>)</span></code></pre></div>
<p>Not all matched keypoint pairs are correct. Incorrect matches lead to inaccurate transform. We can decide if a match is correct or not by checking whether the pairs are close enough after transformation. This is exactly what <code>cv2.USAC_ACCURATE</code> method does. The parameter <code>ransacReprojThreshold</code> is maximum allowed reprojection error to treat a point pair as correct. In above code, the maximum allowed reprojection error is 3 pixels.</p>
<p>The return value <code>status</code> indicates the correctness of keypoints. <code>status[i]==1</code> means <code>src_kps[i]</code> and <code>dst_kps[i]</code> are taken as a correct pair.</p>
<p>The return value <code>T</code> is a <span class="math inline">\(3\times3\)</span> transform matrix <span class="math display">\[\begin{equation}
T = \begin{bmatrix}
h_{11} &amp; h_{12} &amp; h_{13} \\
h_{21} &amp; h_{22} &amp; h_{23} \\
h_{31} &amp; h_{32} &amp; h_{33} \\
\end{bmatrix},
\end{equation}\]</span> which transforms a point at <span class="math inline">\((x,y)\)</span> to location <span class="math inline">\((x&#39;, y&#39;)\)</span>: <span class="math display">\[\begin{equation}
\left\{
\begin{matrix}
x&#39; =\dfrac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}\\
y&#39; =\dfrac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}
\end{matrix} \right.
\end{equation}\]</span></p>
<p>We can apply the transformation <code>T</code> to image 2 by <code>cv2.warpPerspective</code>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>H, W, _ <span class="op">=</span> img2.shape</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> W<span class="op">*</span><span class="dv">2</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>new_img2 <span class="op">=</span> cv2.warpPerspective(</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                    src   <span class="op">=</span> img2,</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                    M     <span class="op">=</span> T,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>                    dsize <span class="op">=</span> (W, H),</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>                    dst   <span class="op">=</span> np.zeros_like(img),</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>                    borderMode <span class="op">=</span> cv2.BORDER_TRANSPARENT)</span></code></pre></div>
<p><code>dsize</code> specifies the size of transformed image.</p>
<center>
<a href="image/md/new_img2.jpg"> <img alt="transformed image 2" style="width:90%" src="image/md/new_img2.jpg"/> </a>
<p>
Fig 3. transformed image 2.
</p>
<a href="image/md/new_img1.jpg"> <img alt="transformed image 2" style="width:90%" src="image/md/new_img1.jpg"/> </a>
<p>
Fig 4. resized image 1.
</p>
</center>
<p>We also need to pad image 1 with zeros to make have same size with transformed image 2.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># resize img1</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>new_img1 <span class="op">=</span> np.hstack([img1, np.zeros_like(img1)])</span></code></pre></div>
<h3 data-number="4.3.2" id="stack-two-image-together"><span class="header-section-number">4.3.2</span> Stack two image together</h3>
<p>The last step is to stack them together. Direct average gives following result.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>direct_mean <span class="op">=</span> new_img1<span class="op">/</span><span class="dv">2</span> <span class="op">+</span> new_img2<span class="op">/</span><span class="dv">2</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>imshow(<span class="st">&#39;direct_mean.jpg&#39;</span>, direct_mean)</span></code></pre></div>
<center>
<a href="image/md/direct_mean.jpg"> <img alt="direct mean" style="width:90%" src="image/md/direct_mean.jpg"/> </a>
<p>
Fig 5. Direct mean.
</p>
</center>
<p>Actually, we need to take average only for the overlapped part. For unoverlapped part, we should copy the pixel value from image 1 or image 2. This could be done by following code:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># smart average</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">=</span> np.zeros([H,W,<span class="dv">1</span>]) <span class="op">+</span> <span class="fl">1e-10</span>     <span class="co"># add a tiny value to avoid ZeroDivisionError</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">+=</span> (new_img2 <span class="op">!=</span> <span class="dv">0</span>).<span class="bu">any</span>(<span class="dv">2</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">+=</span> (new_img1 <span class="op">!=</span> <span class="dv">0</span>).<span class="bu">any</span>(<span class="dv">2</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to floating number to avoid overflow</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>new_img1 <span class="op">=</span> np.float32(new_img1)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>new_img2 <span class="op">=</span> np.float32(new_img2)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>stack <span class="op">=</span> (new_img2<span class="op">+</span>new_img1)<span class="op">/</span>cnt</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>imshow(<span class="st">&#39;stack.jpg&#39;</span>, stack)</span></code></pre></div>
<p><code>cnt</code> counts how many images have a valid pixel at <code>(i,j)</code>:</p>
<ul>
<li><code>cnt[i,j]</code> is 2, for overlapped part.</li>
<li><code>cnt[i,j]</code> is 1, if only one image have a valid pixel at <code>(i,j)</code>.</li>
<li><code>cnt[i,j]</code> is 0, if no image have a valid pixel at <code>(i,j)</code>.</li>
</ul>
<center>
<a href="image/md/stack.jpg"> <img alt="Smart average" style="width:90%" src="image/md/stack.jpg"/> </a>
<p>
Fig 5. Smart average.
</p>
</center>
<h1 data-number="5" id="panorama-stitching"><span class="header-section-number">5</span> Panorama stitching</h1>
<p>Last section introduced how to stitch two images into a large image. In this section, we learn how to stitch a video into a real panorama. First, we read in all frames of <code>image/Vcore.mov</code>. I provide a utility function <code>read_video_frames</code> in <code>utils.py</code> for you to load all frames from a video.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> read_video_frames</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>video_name <span class="op">=</span> <span class="st">&#39;image/Vcore.mov&#39;</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> read_video_frames(video_name)</span></code></pre></div>
<p>The panorama is usually several times larger than video frames. For this video, we let <code>(H, W) = (h*4, w*3)</code>.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>h, w <span class="op">=</span> images[<span class="dv">0</span>].shape[:<span class="dv">2</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>H, W <span class="op">=</span> h<span class="op">*</span><span class="dv">4</span>, w<span class="op">*</span><span class="dv">3</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>panorama <span class="op">=</span> np.zeros([H,W,<span class="dv">3</span>]) <span class="co"># use a large canvas</span></span></code></pre></div>
<p>Then we initialize our panorama with first frame. For this video, since V Core is scanned from its bottom right corner, so we place first frame at the bottom right corner of our panorama.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># init panorama</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>h_start <span class="op">=</span> H<span class="op">-</span>h<span class="op">-</span>h<span class="op">//</span><span class="dv">2</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>w_start <span class="op">=</span> W<span class="op">-</span>w</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>panorama[h_start:h_start<span class="op">+</span>h, w_start:w_start<span class="op">+</span>w, :] <span class="op">=</span> images[<span class="dv">0</span>]</span></code></pre></div>
<p>Then we align all frames to this initial panorama one by one as last section did. Similarly, we need to maintain a <code>cnt</code> variable to record the count for each pixel and a <code>sum</code> variable to record the sum. To reduce computation, we may only stitch every forth or sixth frames.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>trans_sum <span class="op">=</span> np.zeros([H,W,<span class="dv">3</span>])</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">=</span> np.ones([H,W,<span class="dv">1</span>])<span class="op">*</span><span class="fl">1e-10</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> img <span class="kw">in</span> tqdm(images[::<span class="dv">4</span>], <span class="st">&#39;processing&#39;</span>):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: write your own code here as last section</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       to align img with current panorama</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    aligned_img <span class="op">=</span> ...</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># combine</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    trans_sum <span class="op">+=</span> aligned_img</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    cnt <span class="op">+=</span> (trans <span class="op">!=</span> <span class="dv">0</span>).<span class="bu">any</span>(<span class="dv">2</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    panorama <span class="op">=</span> trans_sum<span class="op">/</span>cnt</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># show</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    imshow(<span class="st">&#39;panorama.jpg&#39;</span>, panorama)</span></code></pre></div>
<center>
<img style="width:32%" src="image/md/panorama_init.jpg"/> <img style="width:32%" src="image/md/panorama_half.jpg"/> <img style="width:32%" src="image/md/panorama.jpg"/>
<p>
Fig 6. From left to right: initial panorama; stitching half of frames; stitching all frames.
</p>
</center>
<h1 data-number="6" id="video-stabilization-and-stack-denoising"><span class="header-section-number">6</span> Video stabilization and stack denoising</h1>
<p>Sometimes, the images we shoot contains lots of random noise, especially in dark environment. The most commonly seen noise is white noise as shown below.</p>
<center>
<table>
<tr>
<td>
<img style="height: 160px;" src="image/md/noisy.jpg"/>
</td>
<td style="vertical-align: middle;">
=
</td>
<td>
<img style="height: 160px;" src="image/md/original.jpg"/>
</td>
<td style="vertical-align: middle;">
+
</td>
<td>
<img style="height: 160px;" src="image/md/noise.png"/>
</td>
</tr>
</table>
<p>
Fig 7. noisy image = scene + noise
</p>
</center>
<p>Noisy image can be considered as a combination of scene and noise. One way to reduce noise is to shoot multiple images or a video for the same scene, then ‘stack’ all images or frames together to get a clear image. Assume we have <span class="math inline">\(n\)</span> images/frames shot for the same scene: <span class="math display">\[
\begin{matrix}
\text{img}_1 = \text{scene} + \text{noise}_1 \\
\text{img}_2 = \text{scene} + \text{noise}_2 \\
\vdots\\
\text{img}_n = \text{scene} + \text{noise}_4 \\
\end{matrix}
\]</span></p>
<p>If we average all them together. The scene itself will be reserved and noises will cancel each other: <span class="math display">\[\begin{align*}
\frac{1}{N}\sum_i \text{img}_i
                    &amp;= \text{scene} + \frac{1}{N}\sum_i \text{noise}_i \\
                    &amp;\approx \text{scene}
\end{align*}\]</span></p>
<p>This is the basic idea of stack denoising. However, in reality, the multiple images are usually shot not from exactly same location or direction. The video we shot are also not stable enough. Directly stacking them leads to a blur image. So we need to align images or stabilise the video before stacking. The whole procedure contains two step: 1. align images (algorithm stabilization). 2. average them.</p>
<p>which is same as panorama stitching. Algorithm stabilization is an alternative to tripod. If you do not want to carry a heavy and big tripod with you everywhere, algorithm stabilization is a good alternative.</p>
<center>
<img style="width:30%" src="image/md/tripod.jpg"/>
<p>
Tripod
</p>
</center>
<h2 data-number="6.1" id="long-exposure"><span class="header-section-number">6.1</span> Long exposure</h2>
<p>The first example is long exposure. Long exposure is good at capturing dark scene. Its mechanism is similar as stack denoising. However, sometimes long exposure is not enough either, especially in extreme dark environment. In this case, we could shot multiple images for the same scene and and stack them to get a clearer image.</p>
<p>The right most image below staks 10 images. Each of them is exposed for 3 seconds, so the stacked image is equivalent to 30 seconds exposure.</p>
<center>
<a href="image/md/night.jpg"> <img style="width:23%" src="image/md/night.jpg"/></a> <a href="image/night/0.jpg"> <img style="width:23%" src="image/night/0.jpg"/></a> <a href="image/md/night_mean.jpg"> <img style="width:23%" src="image/md/night_mean.jpg"/></a> <a href="image/md/night_stabilized_mean.jpg"> <img style="width:23%" src="image/md/night_stabilized_mean.jpg"/></a>
<figcaption>
Fig 6. From left to right: (a) no long exposure. (b) 3s long exposure. <br/> (c) stack 10 long-exposure images without stabilization. <br/> (d) stack 10 long-exposure images after stabilization.
</figcaption>
</center>
<h2 data-number="6.2" id="deraining"><span class="header-section-number">6.2</span> Deraining</h2>
<p>Stack denoising can also be use to remove rain drops, if we consider rain drops as a kind of noise. It can produce a better result than the one produced by median filter. Median filter is a kind of single-image deraining technique. The main drawback of median filter is that it not only removes rain drops but also blur the image. Stack denoising overcomes this drawback, which can produce clear rain-free result, but require multiple images as input. Following figures compare results of median filter and stack denoising.</p>
<center>
<a href="image/md/frame0.jpg"> <img style="width:45%" src="image/md/frame0.jpg"/></a> <a href="image/md/mean.jpg"> <img style="width:45%" src="image/md/mean.jpg"/></a>
<figcaption>
Fig 7. (a) rainy image. (b) average without stabilization.
</figcaption>
<a href="image/md/median.jpg"> <img style="width:45%" src="image/md/median.jpg"/></a> <a href="image/md/stabilized_mean.jpg"> <img style="width:45%" src="image/md/stabilized_mean.jpg"/></a>
<figcaption>
Fig 7. (c) median filter. (d) average after stabilization (stack denoising).
</figcaption>
</center>
<h1 data-number="7" id="assignment-12-points-2-bonus-points"><span class="header-section-number">7</span> Assignment (12 points + 2 bonus points)</h1>
<h2 data-number="7.1" id="image-alignment-via-sift-9-points"><span class="header-section-number">7.1</span> Image alignment via SIFT (9 points)</h2>
<p>First task is to perform keypoints detection and matching on <code>image/left2.mov</code> and <code>image/right2.jpg</code>. Please complete <code>sift.py</code> to finish this task.</p>
<p>Steps:</p>
<ol type="1">
<li><p>Read in first and last frame of video <code>image/rain2.mov</code>. Hereafter, we refer them as <code>img1</code> and <code>img2</code> respectively.</p></li>
<li><p>Convert them to grayscale image, and save the converted images as <code>img1_gray.jpg</code> and <code>img2_gray.jpg</code>. (1 point)</p></li>
<li><p>Detect keypoints of two images via SIFT, draw keypoints on them and save results as <code>img1_keypoints.jpg</code> and <code>img2_keypoints.jpg</code>. (2 points)</p></li>
<li><p>Match keypoints between two images, draw matched keypoints pairs, and save as <code>match.jpg</code>. (2 points: 1 for matching, 1 for Lowe’s ratio test)</p></li>
<li><p>Obtain the transform matrix from <code>img2</code> to <code>img1</code> by <code>cv2.findHomography</code>. Print the transform matrix, and <strong>include its value in your report</strong>. (1 point)</p></li>
<li><p>Apply the transform matrix to <code>img2</code>, and save transformed result as <code>transformed.jpg</code>. (1 point)</p></li>
<li><p>Stack them into one image, and save as <code>stack.jpg</code>. (2 point)</p></li>
</ol>
<h2 data-number="7.2" id="panorama-stitching-3-points"><span class="header-section-number">7.2</span> Panorama stitching (3 points)</h2>
<p>Third task is to stitch all frames of video <code>image/wall_paint.mov</code> into a panorama. Please complete <code>utils.py</code> to finish this task.</p>
<p>Steps:</p>
<ol type="1">
<li>read in all frames of <code>image/wall_paint.mov</code>.</li>
<li>align all frames as above task.</li>
<li>average them into one panorama, and save as <code>panorama.jpg</code>. (3 points)</li>
</ol>
<h2 data-number="7.3" id="video-stabilization-and-deraining-2-bonus-points"><span class="header-section-number">7.3</span> Video stabilization and deraining (2 bonus points)</h2>
<p>Second task is to stabilize video <code>image/rain2.MOV</code> such that we can stack all image together to get a rain-free image. Please complete <code>derain.py</code> to finish this task.</p>
<p>Steps:</p>
<ol type="1">
<li>read in all frames of <code>image/rain2.MOV</code>.</li>
<li>stabilize the video (align all frames with first one), and save stabilized video as <code>stabilized.mp4</code>. Tip: Function <code>write_frames_to_video</code> in <code>utils.py</code> can help you save a list of frames as a video. (1 point)</li>
<li>average stabilized frames into one image, and save as <code>stabilized_mean.jpg</code>. (1 point)</li>
</ol>
<h2 data-number="7.4" id="submission-instruction"><span class="header-section-number">7.4</span> Submission instruction</h2>
<p>Your submission should include</p>
<ol type="1">
<li>A report contains all intermediate results (images you saved).</li>
<li>All source code that could reproduce your results. (<code>*.py</code>).</li>
<li>If your submission is too large to upload to BlackBoard, you may give an external link to your intermediate results instead of uploading them to BlackBoard.</li>
</ol>
<p>Note: <code>cv2.createStitcher</code> and <code>cv2.Stitcher_create</code> are not allowed to use in this assignment.</p>
<p>Please submit before <strong>23:59 on November 14 (Sunday)</strong>. You may submit as many times as you want, but only your latest submission will be graded.</p>
<h1 class="unnumbered" id="references">References</h1>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html">Feature Detection and Description – OpenCV-Python Tutorials beta documentation</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Harris, C., &amp; Stephens, M. (1988, August). A combined corner and edge detector. In Alvey vision conference (Vol. 15, No. 50, pp. 10-5244).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Bay, H., Tuytelaars, T., &amp; Van Gool, L. (2006, May). Surf: Speeded up robust features. In European conference on computer vision (pp. 404-417). Springer, Berlin, Heidelberg.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Rublee, E., Rabaud, V., Konolige, K., &amp; Bradski, G. (2011, November). ORB: An efficient alternative to SIFT or SURF. In 2011 International conference on computer vision (pp. 2564-2571). IEEE.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
